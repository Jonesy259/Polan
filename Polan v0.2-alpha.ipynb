{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if empty lists makes difference.\n",
    "# Perhaps remove section where it references to wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Sat Jan 07 00:00:06 +0000 2017\", \"default_profile\": true, \"default_profile_image\": true, \"friends_count\": 101, \"id\": 817521154847969280, \"id_str\": \"817521154847969280\", \"lang\": \"en\", \"name\": \"Josh Jones\", \"profile_background_color\": \"F5F8FA\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"Jonesy_259\"}\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Accept search term from user and download\n",
    "# the last 100 tweets from that search term\n",
    "#######################\n",
    "\n",
    "# Install the python-twitter module. Provides python like interface to the Twitter API. \n",
    "# Twitter API is a REST API, it provides information in the form of a JSON which\n",
    "# will need to be parsed- python-twitter will do this.\n",
    "\n",
    "import twitter\n",
    "\n",
    "# Call pydoc twitter.api at the anaconda terminal to access documentation. \n",
    "\n",
    "# Twitter API keys are needed to access tweets - both from feature vector and \n",
    "# tweets for classification.\n",
    "\n",
    "api = twitter.Api(consumer_key='6b9ebwaNU4DDa9G9xF1FhrZQt',\n",
    "                 consumer_secret='2prgddykMj2b7b9zTeN78BBhdrgdaNxjtSyyoo8iNRzKAZhzMX',\n",
    "                 access_token_key='817521154847969280-bc6J796tc0cRjlhigiRZIoQVIzeW2Hf',\n",
    "                 access_token_secret='WAZy2gZ9Ok8NdP3W8TOMNliSUUGrLjesudvqA3nEEh9wH')\n",
    "\n",
    "print api.VerifyCredentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for: '@realDonaldTrump'\n",
      "num tweets: 100 term: @realDonaldTrump\n"
     ]
    }
   ],
   "source": [
    "# Function accepts search term and then fetches the tweets for that term\n",
    "def createTestData(search_string):\n",
    "    try:\n",
    "        tweets_fetched=api.GetSearch(search_string, count=100)\n",
    "        # This will return a list with twitter.Status objects. These include\n",
    "        # text, hashtags etc of the tweets that are fetched. \n",
    "        print \"num tweets: \"+str(len(tweets_fetched))+\" term: \"+search_string\n",
    "        # Since these tweets don't have sentiment labels yet we will \n",
    "        # keep the label empty\n",
    "        return [{\"text\":status.text, \"label\":None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print \"Error\"\n",
    "        return None\n",
    "    \n",
    "search_string=input(\"Search for: \")\n",
    "testData=createTestData(search_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': None,\n",
       "  'text': u'Today the media hyperventilated when @realDonaldTrump teased the monthly Jobs Report.\\nThey dared us to find a presi\\u2026 https://t.co/JD2cZd7Ujr'},\n",
       " {'label': None,\n",
       "  'text': u'.@realDonaldTrump is smiling next to a man who runs a gulag jailing some 200,000 North Koreans and who oversaw the\\u2026 https://t.co/FI5VCSqk4o'},\n",
       " {'label': None,\n",
       "  'text': u'In just two years, @realDonaldTrump has done more for the black community than any President in modern history, inc\\u2026 https://t.co/yDWRfaHVzI'},\n",
       " {'label': None,\n",
       "  'text': u'RT @realDonaldTrump: Real @FoxNews is doing great, Fake News CNN is dead! https://t.co/1p37tPiB3v'},\n",
       " {'label': None,\n",
       "  'text': u'RT @oldsourpuss: @jbmartinjbmart @steve_vladeck @realDonaldTrump They\\u2019re claiming Trump couldn\\u2019t have obstructed justice because he didn\\u2019t\\u2026'},\n",
       " {'label': None,\n",
       "  'text': u\"RT @Real_Gaz: John Brennan says @realDonaldTrump is paranoid and insecure\\n\\nWell if that ain't the pot calling the kettle black\\n\\nThat's like\\u2026\"},\n",
       " {'label': None,\n",
       "  'text': u\"RT @MrsPerrin: Re: #Vets Veteran's Choice / Mission Program\\nSometime around June 6, @realDonaldTrump will sign the new plan.\\nIt's basically\\u2026\"},\n",
       " {'label': None,\n",
       "  'text': u'RT @realDonaldTrump: Looking forward to seeing the employment numbers at 8:30 this morning.'},\n",
       " {'label': None,\n",
       "  'text': u'RT @DrDenaGrayson: Mueller is closing in on @realDonaldTrump.\\U0001f60e\\n\\nTeam\\U0001f1f7\\U0001f1faTrump\\u2014aided &amp; abetted by Devin Nunes &amp; others\\u2014is making a last ditch\\u2026'},\n",
       " {'label': None,\n",
       "  'text': u'RT @TomFitton: Why are Republicans like Gowdy making excuses for Spygate targeting of @RealDonaldTrump?  To protect Mueller!  Plus @Judicia\\u2026'},\n",
       " {'label': None,\n",
       "  'text': u'UK OWES @realDonaldTrump AN APOLOGY. \\nDISGRACEFUL MEDDLING. https://t.co/YjMormsREC'},\n",
       " {'label': None,\n",
       "  'text': u'Bawahahahaha \\n@realDonaldTrump https://t.co/291kVHEAIs'},\n",
       " {'label': None,\n",
       "  'text': u'RT @LadyRedWave: Personally, as a #TrumpSupporter I had no doubts @realDonaldTrump could turn this country around! He has &amp; will always hav\\u2026'},\n",
       " {'label': None,\n",
       "  'text': u'RT @Biafran16Son: #FreeBiafra from Nigeria before its too late #Genocide #Islamisation @realDonaldTrump @SecPompeo @StateDept @FoxNews http\\u2026'},\n",
       " {'label': None,\n",
       "  'text': u'RT @realDonaldTrump: Now this is a record that will never be broken! https://t.co/rtpLfvsBU5'},\n",
       " {'label': None,\n",
       "  'text': u'RT @lgera_andrade: Mi cart\\xf3n de hoy en @fronterainfo @realDonaldTrump @POTUS desestabiliza #MercadoGlobal por los impuestos en los #Arancel\\u2026'},\n",
       " {'label': None,\n",
       "  'text': u'RT @realDonaldTrump: When you\\u2019re almost 800 Billion Dollars a year down on Trade, you can\\u2019t lose a Trade War! The U.S. has been ripped off\\u2026'},\n",
       " {'label': None,\n",
       "  'text': u\"@realDonaldTrump Why is it that the #POTUS does not understand how capital letters work?\\n\\nBecause he's a fucking moron.\"},\n",
       " {'label': None,\n",
       "  'text': u'RT @AnthemRespect: There has never been a better time to be an ally of America.\\n\\nThere has never been a worse time to be an enemy of Americ\\u2026'},\n",
       " {'label': None,\n",
       "  'text': u'@realDonaldTrump Even your lies no longer make sense.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Create Training Corpus\n",
    "#######################\n",
    "\n",
    "# Using Sentiment 140 Training Corpus, has 1,600,000 classified tweets - based\n",
    "# on whether the tweet used a :) or :( emoticon\n",
    "def createTrainingCorpus(corpusFile):\n",
    "    import csv\n",
    "    corpus=[]\n",
    "    with open(corpusFile, 'rb') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        for row in lineReader:\n",
    "            corpus.append({\"text\":row[5],\"label\":int(row[0])})\n",
    "    \n",
    "    # Only 20,000 positive and negative tweets respectively. Processing\n",
    "    # 1,600,000 tweets will take too long, and possibly introduce overfitting\n",
    "    trainingData=[]\n",
    "    for label in [0,4]:\n",
    "        i=1\n",
    "        for tweet in corpus:\n",
    "            if tweet[\"label\"]==label and i<=10000:\n",
    "                trainingData.append(tweet)\n",
    "                i+=1\n",
    "    return trainingData\n",
    "\n",
    "corpusFile=\"/Users/Josh/Desktop/Software Project/trainingCorpus/trainingData1600000.csv\"\n",
    "trainingData=createTrainingCorpus(corpusFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.86916327204\n"
     ]
    }
   ],
   "source": [
    "# Process tweets, remove non-dictionary words, punctuation, links etc.\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['rt', \"'s\", 'i'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        # The list of tweets is a list of dictionaries which has the keys, \"text\" and \"label\"\n",
    "        processedTweets=[]\n",
    "        # Each tuple is a list of words + label.\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        # Convert to lowercase\n",
    "        tweet=tweet.lower()\n",
    "        # RemoveLinks\n",
    "        tweet=re.sub('https?://[A-Za-z0-9./]+','',tweet)\n",
    "        # Remove '@' \n",
    "        tweet=re.sub(r'@[A-Za-z0-9]+','',tweet)\n",
    "        # Replace #word with word\n",
    "        tweet=re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "        # Remove non-letters\n",
    "        tweet=re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "        \n",
    "        tweet=word_tokenize(tweet)\n",
    "        # Converts tweet to list of words\n",
    "        # Remove stopwords\n",
    "        tweet = [word for word in tweet if word not in self._stopwords]\n",
    "        # If word is not in wordnet, remove it.\n",
    "        for index, word in enumerate(tweet):\n",
    "            if len(word) < 2:\n",
    "                tweet[index] = 'i'\n",
    "            var = wn.synsets(word)[:1]\n",
    "            if len(var) < 1:\n",
    "                tweet[index] = 'i'\n",
    "        return [word for word in tweet if word not in self._stopwords]\n",
    "        \n",
    "\n",
    "tweetProcessor=PreProcessTweets()\n",
    "start = timer()\n",
    "ppTrainingData=tweetProcessor.processTweets(trainingData)\n",
    "end = timer()\n",
    "print (end - start)\n",
    "ppTestData=tweetProcessor.processTweets(testData)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store processed tweets in CSV, as  processing tweets takes considerable\n",
    "# time, this will mean we only have to do it once.\n",
    "\n",
    "def storePPTrainingData(ppTrainingData,tweetDataFile):\n",
    "    import csv\n",
    "    with open(tweetDataFile,'wb') as csvfile:\n",
    "        linewriter=csv.writer(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        for tweet in ppTrainingData:\n",
    "            try: \n",
    "                linewriter.writerow([tweet[0],tweet[1]])\n",
    "            except Exception,e:\n",
    "                print e\n",
    "                \n",
    "tweetDataFile=\"/Users/Josh/Desktop/Software Project/trainingCorpus/processedCorpus.csv\"\n",
    "storePPTrainingData(ppTrainingData,tweetDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract features and train classifier, two methods: Naive Bayes + SVM\n",
    "\n",
    "import nltk\n",
    "# Naive Bayes Classifier -Using NLTK's built in classifier\n",
    "\n",
    "# Build Vocabulary\n",
    "def buildVocabulary(ppTrainingData):\n",
    "    all_words=[]\n",
    "    for (words,sentiment) in ppTrainingData:\n",
    "        all_words.extend(words)\n",
    "        # Words are not duped, all words present in corpus, correlates \n",
    "        # to how many times it appears in corpus\n",
    "        wordlist=nltk.FreqDist(all_words)\n",
    "        # Create dictionary for all words with frequency\n",
    "        word_features=wordlist.keys()\n",
    "        # Return unique list of words in corpus, just the keys from\n",
    "        # above dictionary\n",
    "        return word_features\n",
    "\n",
    "# NLTK has an apply_features function that takes a user defined\n",
    "# function to extract features from training data. The following extract feature\n",
    "# function takes each tweet and represents it with the presence or absence\n",
    "# of words in our vocabulary (every word in training data/corpus)\n",
    "\n",
    "def extract_features(tweet):\n",
    "    tweet_words=set(tweet)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word]=(word in tweet_words)\n",
    "        # This will make a disctionary with keys like 'contains word1\n",
    "        # etc, with values True or False\n",
    "    return features\n",
    "\n",
    "# Extract features and train classifier\n",
    "word_features = buildVocabulary(ppTrainingData)\n",
    "trainingFeatures=nltk.classify.apply_features(extract_features,ppTrainingData)\n",
    "# apply_features will take the function extract_features function defined above, \n",
    "# and aply it to each element of ppTrainingData. It automatically identifies that \n",
    "# each of those elements (in ppTrain...) is actually a tuple, so itt takes the first element\n",
    "# of the tuple to be text and the second element to be the label, and applies\n",
    "# the function only to the text\n",
    "\n",
    "NBayesClassifier=nltk.NaiveBayesClassifier.train(trainingFeatures)\n",
    "# We now have classifier that has been trained using NaiveBayes, NLTK library \n",
    "\n",
    "# Support Vector Machine - here we use sentiwordnet to weight words\n",
    "# based on their usage in postive/negative conexts\n",
    "# from nltk.corpus import sentiwordnet as swn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Data form will have to be changed slightly. SKLearn has a CountVectorizer object. \n",
    "# This will take in documents and directly return a term-document matrix with\n",
    "# the frequencies of a word in the document. It builds the vocabulary by itself (takes a tweet \n",
    "# + returns terms with their frequencies). TrainingData and the labels (sentiment) will\n",
    "# be given to the SVM classifier seperately and not as tuples. Another distinguishment\n",
    "# between Naive Bayes is that it is a binary classifier- can only classify data \n",
    "# into two classes, no 'neutral'\n",
    "\n",
    "svmTrainingData=[' '.join(tweet[0]) for tweet in ppTrainingData]\n",
    "# Create sentences out of the lists of words\n",
    "\n",
    "vectorizer=CountVectorizer(min_df=1)\n",
    "X=vectorizer.fit_transform(svmTrainingData).toarray()\n",
    "# X is the term document matrix\n",
    "vocabulary=vectorizer.get_feature_names()\n",
    "\n",
    "# Use sentiwordnet to add weights to features.\n",
    "\n",
    "swn_weights=[]\n",
    "\n",
    "for word in vocabulary:\n",
    "    try:\n",
    "        # Try block as not all words in trainingData will be in sentiwordnet. Look\n",
    "        # for synsets of that word in sentiwordnet\n",
    "        synset=list(swn.senti_synsets(word))\n",
    "        # use the first synset only to compute score, as this will represent\n",
    "        # the most common (meaning) usage of that word\n",
    "        common_meaning=synset[0]\n",
    "        # If the positive score is greater, use that as the weight, vice versa. \n",
    "        if common_meaning.pos_score()>common_meaning.neg_score():\n",
    "            weight=common_meaning.pos_score()\n",
    "        elif common_meaning.pos_score()<common_meaning.neg_score():\n",
    "            weight=-common_meaning.neg_score()\n",
    "        else:\n",
    "            weight=0\n",
    "    except:\n",
    "        weight=0\n",
    "    swn_weights.append(weight)\n",
    "    \n",
    "# Multiply each array in original matrix with the following weights\n",
    "# Initialise a list\n",
    "\n",
    "swn_X=[]\n",
    "for row in X:\n",
    "    swn_X.append(np.multiply(row,np.array(swn_weights)))\n",
    "# Convert the list to a numPy array\n",
    "swn_X=np.vstack(swn_X)\n",
    "\n",
    "# Represent labels as positive=1, negative=2 so that everything is respresented numPy arrays.\n",
    "#labels_to_array={\"positive\":1,\"negative\":2}\n",
    "#labels=[labels_to_array[tweet[1]] for tweet in ppTrainingData]\n",
    "labels=[tweet[1] for tweet in ppTrainingData]\n",
    "y=np.array(labels)\n",
    "\n",
    "# Build SVM Classifier\n",
    "from sklearn.svm import SVC\n",
    "SVMClassifier=SVC()\n",
    "SVMClassifier.fit(swn_X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier on downloaded tweets\n",
    "\n",
    "# NaiveBayes\n",
    "NBResultLabels=[NBayesClassifier.classify(extract_features(tweet[0])) for tweet in ppTestData]\n",
    "\n",
    "# SVM\n",
    "SVMResultLabels=[]\n",
    "for tweet in ppTestData:\n",
    "    tweet_sentence=' '.join(tweet[0])\n",
    "    svmFeatures=np.multiply(vectorizer.transform([tweet_sentence]).toarray(),np.array(swn_weights))\n",
    "    SVMResultLabels.append(SVMClassifier.predict(svmFeatures)[0])\n",
    "    # Append 1 or 2 to the list, as tweet is postive/negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Result Positive Sentiment: 100\n",
      "SVM Result Positive Sentiment: 100\n"
     ]
    }
   ],
   "source": [
    "# Get majority vote + return sentiment\n",
    "\n",
    "# NaiveBayes\n",
    "if NBResultLabels.count('positive')>NBResultLabels.count('negative'):\n",
    "    print \"NB Result Positive Sentiment: \" + str(100*NBResultLabels.count('positive')/len(NBResultLabels))\n",
    "else:\n",
    "     print \"NB Result Negative Sentiment: \" + str(100*NBResultLabels.count('negative')/len(NBResultLabels))\n",
    "        \n",
    "# SVM\n",
    "if SVMResultLabels.count(1)>SVMResultLabels.count(2):\n",
    "    print \"SVM Result Positive Sentiment: \" + str(100*SVMResultLabels.count(1)/len(SVMResultLabels))\n",
    "else:\n",
    "     print \"SVM Result Negative Sentiment: \" + str(100*SVMResultLabels.count(2)/len(SVMResultLabels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
