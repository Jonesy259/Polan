{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Backend Code to Classify Tweets\n",
    "# Using Support Vector Machine Classification\n",
    "\n",
    "#### SciKit Learn Library: http://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Future Features:\n",
    "Save Classifier: http://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libararies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Training Corpus:\n",
    "This corpus has already been preprocessed by create_trainingCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data-set\n",
    "data_set = pd.read_csv(\"processedCorpus.csv\", names=[\"Tweet\", \"Sentiment\"])\n",
    "# This is our independent variable 'X'- The tweet data\n",
    "# Using Pandas library get the tweet text from the first column of csv file\n",
    "# [row,column]\n",
    "X = data_set.iloc[:, :-1]\n",
    "# This is our dependent variable 'y'- Positive Negative\n",
    "# Use Pandas library to get the tweet from the last column\n",
    "y = data_set.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Categroical data (negative and positive) to numerical data 0 and 1\n",
    "# Support Vector CLassifier needs floats to classify data, not strings.\n",
    "# LabelEncoder does encodes the categorical data here two categories(negative, positive)\n",
    "# to numerical data 0 and 1. The categorical data is converted into numerical data alphabetically.\n",
    "# There are three common methods used here: fit, fit_transfrom, transform in this LabelEncoder class\n",
    "# method: fit = > will not modify the data but will convert them into numerical value into memory.\n",
    "# method: transform => will use numerical values from the memory to convert the text data to numerical data.\n",
    "# method: fit_transform  => combines above functions\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(X[\"Tweet\"]) # convert data to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer creates the bag of words model\n",
    "# 1500 most common words is taken, takes in to account term frequency\n",
    "cv = CountVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the text data is converted into bag of words model\n",
    "# bag of words -> https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "# represents training data as the absence or occurance of words in feature vector, 1's and 0's\n",
    "# Use sklearn's libaray to create a bag of words, term document matrix\n",
    "# use numPy to convert bag of words to ndimensional array\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "# Scale the model. Transform data so that the mean value is 0, standard dev of 1\n",
    "# fit -> Calculate mean and standard deviation\n",
    "# transform -> use the values to scale the data as above\n",
    "# fit_transform -> combine the two above steps\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits ndimensional array into random train and test subsets.\n",
    "# Split 80% to training, 20% for testing.\n",
    "# X_train, Y_train, 80% of training data and their values.\n",
    "# X_test, Y_test, remaining 20% of training data and their values.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classification library, scikitlearn.\n",
    "SVMClassifier=SVC()\n",
    "# Train the classifier with trainingdata\n",
    "SVMClassifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Accuracy\n",
    "For help understanding the confusion matrix- a tool analysising accuracy:\n",
    "https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict whether the test data is positive or negative\n",
    "Y_pred = SVMClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prediction of the test data with the actual labels.\n",
    "# Accuracy = amount that is correct/total amount\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix to analyse accuracy\n",
    "# Actual 0,1 being columns\n",
    "# Predicted 0,1 being rows\n",
    "# See https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Classifier\n",
    "Data entered here is not preprocessed, so may not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "print(\"Predict The sentiment\")\n",
    "data = input(\"Enter your data to get the sentiment: \")\n",
    "data = [data]\n",
    "# Convert data to bag of words model, put that\n",
    "# in ndimensional numPy array\n",
    "array = cv.transform(data).toarray()\n",
    "\n",
    "r = SVMClassifier.predict(array)\n",
    "# 0 is negative, 1 is positive\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get Test Data\n",
    "Using python-twitter library to grab tweets - this avoids the annoyance of processing .JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API keys are needed to access tweets - both from feature vector and \n",
    "# tweets for classification.\n",
    "api = twitter.Api(consumer_key='6b9ebwaNU4DDa9G9xF1FhrZQt',\n",
    "                 consumer_secret='2prgddykMj2b7b9zTeN78BBhdrgdaNxjtSyyoo8iNRzKAZhzMX',\n",
    "                 access_token_key='817521154847969280-bc6J796tc0cRjlhigiRZIoQVIzeW2Hf',\n",
    "                 access_token_secret='WAZy2gZ9Ok8NdP3W8TOMNliSUUGrLjesudvqA3nEEh9wH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw query to search terms via data\n",
    "# see: http://python-twitter.readthedocs.io/en/latest/searching.html\n",
    "'''\n",
    "Donald Trump = \"l=&q=Donald%20Trump%20since%3A2018-06-05%20until%3A2018-06-06&count=100\"\n",
    "Hillary C = \"l=&q=hillary%20clinton%20since%3A2018-06-05%20until%3A2018-06-06&count=100\"\n",
    "Barack Obama = \"l=&q=barack%20obama%20since%3A2018-06-05%20until%3A2018-06-06&count=100\"\n",
    "Pope Francis = \"l=&q=pope%20francis%20pontifex%20since%3A2018-06-05%20until%3A2018-06-06&count=100\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tweets, remove non-dictionary words, punctuation, links etc.\n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['rt', \"'s\", 'i'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        # The list of tweets is a list of dictionaries which has the keys, \"text\" and \"label\"\n",
    "        processedTweets=[]\n",
    "        # Each tuple is a list of words + label.\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweet=self._processTweet(tweet)\n",
    "            if len(processedTweet) > 0:\n",
    "                processedTweets.append(processedTweet)  \n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        # Convert to lowercase\n",
    "        tweet=tweet.lower()\n",
    "        # RemoveLinks\n",
    "        tweet=re.sub('https?://[^\\s]+','',tweet)\n",
    "        # Remove '@' \n",
    "        tweet=re.sub(r'@[^\\s]+','',tweet)\n",
    "        # Replace #word with word\n",
    "        tweet=re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "        # Remove non-letters - unicode, random numbers\n",
    "        tweet=re.sub(\"[^a-z]\", \" \",tweet)\n",
    "        # Converts tweet to list of words\n",
    "        tweet=word_tokenize(tweet)\n",
    "        # Stem the words\n",
    "        # Stemming is the process of converting words into their root form\n",
    "        # For example: loving, loved will be converted to love\n",
    "        stemmer=PorterStemmer()\n",
    "        tweet=[stemmer.stem(word) for word in tweet]\n",
    "        stripper = lambda word: word.strip()\n",
    "        tweet = list(map(stripper, tweet))\n",
    "        tweet = filter(None, tweet)\n",
    "        # Remove stopwords\n",
    "        tweet=[word for word in tweet if word not in self._stopwords]\n",
    "        tweet= \" \".join(tweet)\n",
    "        return tweet\n",
    "        # Old code to remove words that aren't in dict, but decided against as\n",
    "        # this may reduce sentimental emotional words- eg. haha\n",
    "        '''\n",
    "        # If word is not in wordnet, remove it.    \n",
    "        for index, word in enumerate(tweet):\n",
    "            if len(word) < 3:\n",
    "                tweet[index] = 'i'\n",
    "            var = wn.synsets(word)[:1] \n",
    "            if len(var) < 1:\n",
    "                tweet[index] = 'i'\n",
    "        # Rerun stopwords check as words that weren't in the dict were replaced\n",
    "        # with 'i'... part of stopwords list\n",
    "        return [word for word in tweet if word not in self._stopwords] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment():\n",
    "    dates = ['06-06', '06-05', '06-04', '06-03', '06-02', '06-01', '05-31', '05-30', '05-29', '05-28', '05-27']\n",
    "    count = 1\n",
    "    sentimentData = []\n",
    "    while count < len(dates):\n",
    "        datePrev=dates[count]\n",
    "        dateCurrent=dates[count-1]\n",
    "        count += 1\n",
    "        sentimentData.append([dateCurrent, _determineSentiment(dateCurrent, datePrev)])\n",
    "    return sentimentData\n",
    "    \n",
    "\n",
    "def _determineSentiment(dateCurrent, datePrev):\n",
    "    tweetList=[]\n",
    "    # try-catch block to avoid errors\n",
    "    try:\n",
    "        # For loop possibility to search for more than 100 tweets\n",
    "        for counter in range(1):\n",
    "            tweets_fetched=api.GetSearch(raw_query=\"l=&q=pope%20francis%20pontifex%20since%3A2018-\"+datePrev+\"%20until%3A2018-\"+dateCurrent+\"&count=100\")\n",
    "            # This will return a list with twitter.Status objects. These include\n",
    "            # text, hashtags etc of the tweets that are fetched.\n",
    "            for status in tweets_fetched:\n",
    "                tweetList.append(status.text)\n",
    "        #print(\"num tweets: \"+str(len(tweetList)))\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    testData=tweetList\n",
    "    \n",
    "    # Preprocess tweets\n",
    "    tweetProcessor=PreProcessTweets()\n",
    "    ppTestData=tweetProcessor.processTweets(testData)\n",
    "\n",
    "    \n",
    "    # Run the classifier on downloaded tweets\n",
    "    # convert test data to bag of words model\n",
    "    # create ndimensional matrix from model using numPy\n",
    "    # scale the bag of words so standard dev is 1, mean is 0.\n",
    "    ResultLabels=[]\n",
    "    ppTestData = set(ppTestData)\n",
    "    for tweet in ppTestData:\n",
    "        Features=cv.transform([tweet]).toarray()\n",
    "        Features=sc.transform(Features)\n",
    "        ResultLabels.append(SVMClassifier.predict(Features)[0])\n",
    "\n",
    "    # Get sentiment positivity\n",
    "    return (100*ResultLabels.count(1)/len(ResultLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentData=getSentiment()\n",
    "print sentimentData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment CSV files for each file are made manually... \n",
    "\n",
    "#### Return the elements of the SVM Labels, where the sentiment is positive. These values are then found in the testData to obeserve the accuracy against the testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "indexElements = []\n",
    "while counter < len(ResultLabels):\n",
    "    if ResultLabels[counter] == 1:\n",
    "        indexElements.append(counter)\n",
    "\n",
    "print indexElements"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
