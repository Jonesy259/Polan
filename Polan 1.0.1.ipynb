{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "## Backend Code to Classify Tweets\n",
    "## Using Support Vector Machine Classification\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save Classifier \n",
    "## http://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import all the necessary libararies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Import Training Corpus\n",
    "############\n",
    "# Get the data-set\n",
    "data_set = pd.read_csv(\"processedCorpus.csv\", names=[\"Tweet\", \"Sentiment\"])\n",
    "# This is our independent variable 'X'- The tweet data\n",
    "# Using Pandas library get the tweet text from the first column of csv file\n",
    "# [row,column]\n",
    "X = data_set.iloc[:, :-1]\n",
    "# This is our dependent variable 'y'- Positive Negative\n",
    "# Use Pandas library to get the tweet from the last column\n",
    "y = data_set.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Categroical data (negative and positive) to numerical data 0 and 1\n",
    "# Support Vector CLassifier needs floats to classify data, not strings.\n",
    "# LabelEncoder does encodes the categorical data here two categories(negative, positive)\n",
    "# to numerical data 0 and 1. The categorical data is converted into numerical data alphabetically.\n",
    "# There are three common methods used here: fit, fit_transfrom, transform in this LabelEncoder class\n",
    "# method: fit = > will not modify the data but will convert them into numerical value into memory.\n",
    "# method: transform => will use numerical values from the memory to convert the text data to numerical data.\n",
    "# method: fit_transform  => does both fit and transform\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(X[\"Tweet\"]) # convert data to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer creates the bag of words model\n",
    "# 1500 most common words is taken, takes in to account term frequency\n",
    "cv = CountVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# All the text data is converted into bag of words model\n",
    "# bag of words -> https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "# represents training data as the absence or occurance of words in feature vector, 1's and 0's\n",
    "# Use sklearn's libaray to create a bag of words, term document matrix\n",
    "# use numPy to convert bag of words to ndimensional array\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "# Scale the model. Transform data so that the mean value is 0, standard dev of 1\n",
    "# fit -> Calculate mean and standard deviation\n",
    "# transform -> use the values to scale the data as above\n",
    "# fit_transform -> combine the two above steps\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits ndimensional array into random train and test subsets.\n",
    "# Split 80% to training, 20% for testing.\n",
    "# X_train, Y_train, 80% of training data and their values.\n",
    "# X_test, Y_test, remaining 20% of training data and their values.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Classification library, scikitlearn.\n",
    "SVMClassifier=SVC()\n",
    "# Train the classifier with trainingdata\n",
    "SVMClassifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict whether the test data is positive or negative\n",
    "Y_pred = SVMClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6862745098039216"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare prediction of the test data with the actual labels.\n",
    "# Accuracy = amount that is correct/total amount\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[658, 325],\n",
       "       [299, 707]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix to analyse accuracy\n",
    "# Actual 0,1 being columns\n",
    "# Predicted 0,1 being rows\n",
    "# See https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict The sentiment\n",
      "Enter your data to get the sentiment: 'unhappy sad kill'\n",
      "[1]\n",
      "['unhappy sad kill']\n"
     ]
    }
   ],
   "source": [
    "# Test prediction\n",
    "print(\"Predict The sentiment\")\n",
    "data = input(\"Enter your data to get the sentiment: \")\n",
    "data = [data]\n",
    "# Convert data to bag of words model, put that\n",
    "# in ndimensional numPy array\n",
    "array = cv.transform(data).toarray()\n",
    "\n",
    "r = SVMClassifier.predict(array)\n",
    "print(r)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "## Get Test Data\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API keys are needed to access tweets - both from feature vector and \n",
    "# tweets for classification.\n",
    "api = twitter.Api(consumer_key='6b9ebwaNU4DDa9G9xF1FhrZQt',\n",
    "                 consumer_secret='2prgddykMj2b7b9zTeN78BBhdrgdaNxjtSyyoo8iNRzKAZhzMX',\n",
    "                 access_token_key='817521154847969280-bc6J796tc0cRjlhigiRZIoQVIzeW2Hf',\n",
    "                 access_token_secret='WAZy2gZ9Ok8NdP3W8TOMNliSUUGrLjesudvqA3nEEh9wH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for: '@HillaryClinton'\n",
      "num tweets: 100 term: @HillaryClinton\n"
     ]
    }
   ],
   "source": [
    "## Function accepts search term and then fetches the tweets for that term\n",
    "def createTestData(search_string):\n",
    "    try:\n",
    "        tweets_fetched=api.GetSearch(search_string, count=500)\n",
    "        # This will return a list with twitter.Status objects. These include\n",
    "        # text, hashtags etc of the tweets that are fetched. \n",
    "        print(\"num tweets: \"+str(len(tweets_fetched))+\" term: \"+search_string)\n",
    "        # Since these tweets don't have sentiment labels yet we will \n",
    "        # keep the label empty\n",
    "        return [status.text for status in tweets_fetched]\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        return None\n",
    "    \n",
    "search_string=input(\"Search for: \")\n",
    "testData=createTestData(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tweets, remove non-dictionary words, punctuation, links etc.\n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['rt', \"'s\", 'i'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        # The list of tweets is a list of dictionaries which has the keys, \"text\" and \"label\"\n",
    "        processedTweets=[]\n",
    "        # Each tuple is a list of words + label.\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweet=self._processTweet(tweet)\n",
    "            if len(processedTweet) > 0:\n",
    "                processedTweets.append(processedTweet)  \n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        # Convert to lowercase\n",
    "        tweet=tweet.lower()\n",
    "        # RemoveLinks\n",
    "        tweet=re.sub('https?://[^\\s]+','',tweet)\n",
    "        # Remove '@' \n",
    "        tweet=re.sub(r'@[^\\s]+','',tweet)\n",
    "        # Replace #word with word\n",
    "        tweet=re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "        # Remove non-letters - unicode, random numbers\n",
    "        tweet=re.sub(\"[^a-z]\", \" \",tweet)\n",
    "        # Converts tweet to list of words\n",
    "        tweet=word_tokenize(tweet)\n",
    "        # Stem the words\n",
    "        # Stemming is the process of converting words into their root form\n",
    "        # For example: loving, loved will be converted to love\n",
    "        stemmer=PorterStemmer()\n",
    "        tweet=[stemmer.stem(word) for word in tweet]\n",
    "        stripper = lambda word: word.strip()\n",
    "        tweet = list(map(stripper, tweet))\n",
    "        tweet = filter(None, tweet)\n",
    "        # Remove stopwords\n",
    "        tweet=[word for word in tweet if word not in self._stopwords]\n",
    "        tweet= \" \".join(tweet)\n",
    "        return tweet\n",
    "        '''\n",
    "        # If word is not in wordnet, remove it.    \n",
    "        for index, word in enumerate(tweet):\n",
    "            if len(word) < 3:\n",
    "                tweet[index] = 'i'\n",
    "            var = wn.synsets(word)[:1] \n",
    "            if len(var) < 1:\n",
    "                tweet[index] = 'i'\n",
    "        # Rerun stopwords check as words that weren't in the dict were replaced\n",
    "        # with 'i'... part of stopwords list\n",
    "        return [word for word in tweet if word not in self._stopwords] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Preprocessor\n",
    "tweetProcessor=PreProcessTweets()\n",
    "ppTestData=tweetProcessor.processTweets(testData)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'trump declar abov law today respons like see democrat dem congress sta', u'probabl becaus like hi favorit candid friend', u'say impeach hear would begun alreadi democrat right', u'thank hi team stakehold join us terrif meet thi', u'trump declar abov law today respons like see democrat dem congress stage walk', u'thi good depress', u'rememb time peopl said go away say aga', u'nachoqueen wash russianvodka', u'kahn wa lawyer kuru queen fukum', u'feliz de que ya sea miembro de voce vital maia', u'say fact', u'democrat rig elect favor sinc begin great republ republican alway fo', u'qanon post click pic pedovor pedogateisr houseofcard ticktock', u'ever fire lie dishonest uneth law violat lawyer ha grate sh', u'excel pardon ex navi sailor sue', u'excel pardon ex navi sailor sue', u'ever fire lie dishonest uneth law violat lawyer ha grate sh', u'surpris america dear involv thi sick mess read surpris g', u'thank hi team stakehold join us terrif meet thi', u'want say thank time appreci done continuin', u'thank hi team stakehold join us terrif meet thi', u'media support idea pardon thing chang fake news', u'said immigr context govern ha never taken stand protect kid thi', u'ever fire lie dishonest uneth law violat lawyer ha grate sh', u'trump declar abov law today respons like see democrat dem congress stage walk', u'democrat rig elect favor sinc begin great republ republican alway fo', u'excel pardon ex navi sailor sue', u'trump declar abov law today respons like see democrat dem congress stage walk', u'thank hi team stakehold join us terrif meet thi', u'democrat rig elect favor sinc begin great republ republican alway fo', u'segunda jornada de mujer l dere del cono sur allavamo programadementoreo vitalvoic', u'thank hi team stakehold join us terrif meet thi', u'excel pardon ex navi sailor sue', u'thank hi team stakehold join us terrif meet thi', u'wrong sydney blumenth start rumor campaign', u'demand see children let make troubl', u'excel pardon ex navi sailor sue', u'excel pardon ex navi sailor sue', u'crook hillari win popular vote stole deduct million illega', u'thank hi team stakehold join us terrif meet thi', u'whatev dude lol old tire', u'thank hi team stakehold join us terrif meet thi', u'excel pardon ex navi sailor sue', u'thank hillari', u'pleas stfu thank', u'anyon make sure take two ever g', u'would good idea young black men women dure black famili would real', u'number one prioriti keep children safe learn environ', u'bet never cancel event due low crowd turnout', u'want say thank time appreci done continuin', u'repli rwnj unbeliev certainli go lon', u'ask happen happen america americafirst mondaymotiv ca', u'surpris america dear involv thi sick mess read surpris g', u'today new buzzword pardon well pardon say say could pardon h', u'one thing separ parent kid anoth elimin parent kid lastli yo', u'ms clinton quietli countri call', u'haaaaa knew full shit eagl academi young men third ave bronx', u'haitian kid got stay parent island keep money', u'zero toler dare enforc law', u'go away lost maga', u'asshol alert', u'democrat rig elect favor sinc begin great republ republican alway fo', u'today new buzzword pardon well pardon say say could pardon h', u'democrat rig elect favor sinc begin great republ republican alway fo', u'parent break law made choic break famili come illeg', u'heisyourpresid', u'ever fire lie dishonest uneth law violat lawyer ha grate sh', u'excel pardon ex navi sailor sue', u'make look like saint would real presid understand', u'thank hi team stakehold join us terrif meet thi', u'yet kind abort syndrom plain unwant viner', u'russia russia russia yawnnnn', u'ok wa bit dig old know daughter b', u'trump declar abov law today respons like see democrat dem congress stage walk', u'import test countri way treat vulner among us especi children', u'excel pardon ex navi sailor sue', u'like noth wors feel helpless thi nation day action children support', u'ever fire lie dishonest uneth law violat lawyer ha grate sh', u'illeg peopl approach border agent reequest asylum', u'thank hi team stakehold join us terrif meet thi', u'would great could separ america', u'remind american peopl w', u'treasonoustraitor', u'lol trump presid win need thank tr', u'lol muller noth', u'offici polici us govern nation immigr separ children famili', u'democrat rig elect favor sinc begin great republ republican alway fo', u'think go chappaqua take pic real presid retweet agre', u'excel pardon ex navi sailor sue', u'excel pardon ex navi sailor sue']\n"
     ]
    }
   ],
   "source": [
    "print(ppTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Positive Sentiment: 69\n"
     ]
    }
   ],
   "source": [
    "# Run the classifier on downloaded tweets\n",
    "ResultLabels=[]\n",
    "ppTestData = set(ppTestData)\n",
    "for tweet in ppTestData:\n",
    "    Features=cv.transform([tweet]).toarray()\n",
    "    Features = sc.transform(Features)\n",
    "    ResultLabels.append(SVMClassifier.predict(Features)[0])\n",
    "    \n",
    "# Get sentiment positivity\n",
    "if ResultLabels.count(1)>ResultLabels.count(0):\n",
    "    print(\"Result Positive Sentiment: \" + str(100*ResultLabels.count(1)/len(ResultLabels)))\n",
    "else:\n",
    "     print(\"Result Negative Sentiment: \" + str(100*ResultLabels.count(0)/len(ResultLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print ResultLabels[:10]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
